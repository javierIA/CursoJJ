{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYMeeusGGrvf"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#Descargar set de datos de MNIST (Numeros escritos a mano, etiquetados)\n",
        "datos, metadatos = tfds.load('mnist', as_supervised=True, with_info=True)\n",
        "\n",
        "#Obtener en variables separadas los datos de entrenamiento (60k) y pruebas (10k)\n",
        "datos_entrenamiento, datos_pruebas = datos['train'], datos['test']\n",
        "\n",
        "#Funcion de normalizacion para los datos (Pasar valor de los pixeles de 0-255 a 0-1)\n",
        "#(Hace que la red aprenda mejor y mas rapido)\n",
        "def normalizar(imagenes, etiquetas):\n",
        "  imagenes = tf.cast(imagenes, tf.float32)\n",
        "  imagenes /= 255 #Aqui se pasa de 0-255 a 0-1\n",
        "  return imagenes, etiquetas\n",
        "\n",
        "#Normalizar los datos de entrenamiento con la funcion que hicimos\n",
        "datos_entrenamiento = datos_entrenamiento.map(normalizar)\n",
        "datos_pruebas = datos_pruebas.map(normalizar)\n",
        "\n",
        "#Agregar a cache (usar memoria en lugar de disco, entrenamiento mas rapido)\n",
        "datos_entrenamiento = datos_entrenamiento.cache()\n",
        "datos_pruebas = datos_pruebas.cache()\n",
        "\n",
        "clases = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQR7QRp_ICYO"
      },
      "source": [
        "#Codigo para mostrar imagenes del set, no es necesario ejecutarlo, solo imprime unos numeros :)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "\n",
        "for i, (imagen, etiqueta) in enumerate(datos_entrenamiento.take(25)):\n",
        "  imagen = imagen.numpy().reshape((28,28))\n",
        "  plt.subplot(5,5,i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(imagen, cmap=plt.cm.binary)\n",
        "  plt.xlabel(clases[etiqueta])\n",
        "\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qOkrBiUJvDi"
      },
      "source": [
        "#Crear el modelo (Ya utiliza capas de convolución y agrupación)\n",
        "#Cuenta con 1 capa de convolución con 32 núcleos y otra con 64. 2 capas de agrupación.\n",
        "#Finalmente una capa densa con 100 neuronas\n",
        "modelo = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), input_shape=(28,28,1), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), #2,2 es el tamano de la matriz\n",
        "\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), #2,2 es el tamano de la matriz\n",
        "\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=100, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "#Compilar el modelo\n",
        "modelo.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwLZOVHUKwUM"
      },
      "source": [
        "#Los numeros de datos de entrenamiento y pruebas (60k y 10k)\n",
        "num_datos_entrenamiento = metadatos.splits[\"train\"].num_examples\n",
        "num_datos_pruebas = metadatos.splits[\"test\"].num_examples\n",
        "\n",
        "#Trabajar por lotes\n",
        "TAMANO_LOTE=32\n",
        "\n",
        "#Shuffle y repeat hacen que los datos esten mezclados de manera aleatoria\n",
        "#para que el entrenamiento no se aprenda las cosas en orden\n",
        "datos_entrenamiento = datos_entrenamiento.repeat().shuffle(num_datos_entrenamiento).batch(TAMANO_LOTE)\n",
        "datos_pruebas = datos_pruebas.batch(TAMANO_LOTE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0tkZyPBLUNs"
      },
      "source": [
        "#Realizar el entrenamiento\n",
        "import math\n",
        "\n",
        "historial = modelo.fit(\n",
        "    datos_entrenamiento,\n",
        "    epochs=60,\n",
        "    steps_per_epoch=math.ceil(num_datos_entrenamiento/TAMANO_LOTE)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blEfn0ZSM6Fq"
      },
      "source": [
        "#Exportar el modelo al explorador! \n",
        "modelo.save('numeros_conv.h5')\n",
        "\n",
        "#Convertirlo a tensorflow.js\n",
        "!pip install tensorflowjs\n",
        "\n",
        "!mkdir carpeta_salida\n",
        "\n",
        "!tensorflowjs_converter --input_format keras numeros_conv.h5 carpeta_salida"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}